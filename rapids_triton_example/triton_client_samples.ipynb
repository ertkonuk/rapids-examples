{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf45364f-bbf7-458c-9c35-e475c102484c",
   "metadata": {},
   "source": [
    "# Example Notebook to show how to use RAPIDS+Pytorch with Triton\n",
    "\n",
    "This notebook calls a ensemble model which uses RAPIDS+Pytorch with Triton\n",
    "\n",
    "\n",
    "<img src=\"notebook_images/ensemble_rapids_simple.jpg\" width=\"300\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2ca165-da53-4319-823a-8bb670386ca0",
   "metadata": {},
   "source": [
    "### Client Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60080a0e-9c82-4290-b4cd-21fc66599817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nvidia-pyindex\n",
    "# !pip install tritonclient[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13897c7a-e6e6-4220-97e3-647f40d51488",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67753be1-6a95-4744-8987-f3d66b997dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import grpc\n",
    "from tritonclient.grpc import service_pb2\n",
    "from tritonclient.grpc import service_pb2_grpc\n",
    "import tritonclient.grpc as grpcclient\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2ee0ed-1ad1-4bfb-86ee-8b9c4787132c",
   "metadata": {},
   "source": [
    "###  Connect to the Triton End to End Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800b69f8-4e40-42f4-a5b0-3ab1df020f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='localhost:8001'\n",
    "\n",
    "triton_client = grpcclient.InferenceServerClient(url=url,verbose=False)\n",
    "\n",
    "channel = grpc.insecure_channel(url)\n",
    "grpc_stub = service_pb2_grpc.GRPCInferenceServiceStub(channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9435c10-adef-4d23-a225-35905d0abafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing_model = 'end_to_end_onnx'\n",
    "preprocessing_model = 'rapids_tokenizer'\n",
    "request = service_pb2.ModelMetadataRequest(name=preprocessing_model,\n",
    "                                           version='1')\n",
    "response = grpc_stub.ModelMetadata(request)\n",
    "print(\"model metadata:\\n{}\".format(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3328c4d8-df21-49d7-b64a-21919fc1fc83",
   "metadata": {},
   "source": [
    "## Send Request to Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a429b4-edc9-4518-9dd5-03df58956c65",
   "metadata": {},
   "source": [
    "### Prepare Input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f88224e-5b55-4f23-8036-ff36ef97d3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_ls = ['The product is great', 'This product is bad','This product is good', 'This product is really bad']*250\n",
    "log_ls = [l.encode('utf-8') for l in log_ls]\n",
    "log_ar = np.array(log_ls).reshape(1,len(log_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e448f01c-39db-4e05-8cba-d1685ef29dd8",
   "metadata": {},
   "source": [
    "### Request Sending Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f482b67e-8ae8-4f46-9131-36b4696ee74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_preprocess_request(log_ar, model_name='rapids_tokenizer'):\n",
    "    triton_client = grpcclient.InferenceServerClient(url=url,verbose=False)\n",
    "    input_grpc = grpcclient.InferInput(\"product_reviews\",log_ar.shape,\"BYTES\")\n",
    "    input_grpc.set_data_from_numpy(log_ar)\n",
    "\n",
    "    outputs = []\n",
    "    outputs.append(grpcclient.InferRequestedOutput('input_ids'))\n",
    "    outputs.append(grpcclient.InferRequestedOutput('attention_mask'))\n",
    "\n",
    "    \n",
    "    output = triton_client.infer(model_name=model_name,\n",
    "                               inputs=[input_grpc],\n",
    "                              outputs=outputs)\n",
    "    \n",
    "\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ad33b0-a115-48c6-a042-c626ac8d4010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_inference_request(log_ar, model_name='end_to_end_pytorch'):\n",
    "    triton_client = grpcclient.InferenceServerClient(url=url,verbose=False)\n",
    "    input_grpc = grpcclient.InferInput(\"product_reviews\",log_ar.shape,\"BYTES\")\n",
    "    input_grpc.set_data_from_numpy(log_ar)\n",
    "    outputs = []\n",
    "    outputs.append(grpcclient.InferRequestedOutput('preds'))\n",
    "    \n",
    "    output = triton_client.infer(model_name=model_name,\n",
    "                               inputs=[input_grpc],\n",
    "                              outputs=outputs)\n",
    "    \n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4bf6c8-722c-4e04-b166-b9c31a7186ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "output = send_preprocess_request(log_ar.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0648eed6-9a3b-4f2c-a876-c13c6769badf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "output = send_inference_request(log_ar,'end_to_end_onnx')\n",
    "#output.as_numpy('preds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c6a8c-877e-4ad0-9aa4-b3fe69a7bfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "output = send_inference_request(log_ar,'end_to_end_pytorch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26d681d-8d82-4dcf-9242-ca2def348a38",
   "metadata": {},
   "source": [
    "##  Predictions\n",
    "\n",
    "##### 1 is positive, 0 is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7492f1c7-d02b-46e4-82fe-6c26c4fbd9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = send_inference_request(log_ar,'end_to_end_pytorch')\n",
    "output.as_numpy('preds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b2a359-4c69-4082-8a17-33aef250aa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = send_inference_request(log_ar,'end_to_end_onnx')\n",
    "output.as_numpy('preds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3895b2",
   "metadata": {},
   "source": [
    "# **added by tugrulkonuk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf95744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e53341",
   "metadata": {},
   "source": [
    "### Huggingface tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f021adfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertTokenizer\n",
    "#tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0d3f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_preprocess_request_hugf(log_ar, model_name='huggingface_tokenizer'):\n",
    "    #triton_client = grpcclient.InferenceServerClient(url=url,verbose=False)\n",
    "    #input_grpc = grpcclient.InferInput(\"product_reviews\", shape=(1,len(txt_ls)), datatype=\"BYTES\")\n",
    "    #log_ls_r = [l.encode('utf-8') for l in log_ls]\n",
    "    #log_ar = np.array(log_ls_r).reshape(1,len(log_ls_r))\n",
    "    #input_grpc = grpcclient.InferInput(\"product_reviews\", shape=np.array(txt_ls).reshape(-1,len(txt_ls)).shape, datatype=\"BYTES\")\n",
    "    #input_grpc.set_data_from_numpy(np.asarray([txt_ls], dtype=object))\n",
    "    triton_client = grpcclient.InferenceServerClient(url=url,verbose=False)\n",
    "    input_grpc = grpcclient.InferInput(\"product_reviews\",log_ar.shape,\"BYTES\")\n",
    "    input_grpc.set_data_from_numpy(log_ar)\n",
    "\n",
    "    outputs = []\n",
    "    outputs.append(grpcclient.InferRequestedOutput('input_ids'))\n",
    "    outputs.append(grpcclient.InferRequestedOutput('attention_mask'))\n",
    "\n",
    "    \n",
    "    output = triton_client.infer(model_name=model_name,\n",
    "                               inputs=[input_grpc],\n",
    "                              outputs=outputs)\n",
    "    \n",
    "\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaa515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_ls = ['The product is great', 'This product is bad','This product is good', 'This product is really bad']*250\n",
    "log_ls = [l.encode('utf-8') for l in log_ls]\n",
    "log_ar = np.array(log_ls).reshape(1,len(log_ls))\n",
    "print(log_ar.shape)\n",
    "output = send_preprocess_request_hugf(log_ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271e41e7",
   "metadata": {},
   "source": [
    "##  Predictions\n",
    "\n",
    "##### 1 is positive, 0 is negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333834db",
   "metadata": {},
   "source": [
    "##### Random inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d149001",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "seq_length = 256\n",
    "input0 = grpcclient.InferInput('input_ids', (batch_size, seq_length), 'INT32')\n",
    "\n",
    "input1 = grpcclient.InferInput('attention_mask', (batch_size, seq_length), 'INT32')\n",
    "input1.set_data_from_numpy(np.ones((batch_size, seq_length), dtype=np.int32))\n",
    "\n",
    "output = grpcclient.InferRequestedOutput('preds')\n",
    "\n",
    "def run_random_inference(model_name='sentiment_model_pytorch'):\n",
    "    triton_client = grpcclient.InferenceServerClient(url=url,verbose=False)\n",
    "    input0.set_data_from_numpy(np.random.randint(10000, size=(batch_size, seq_length), dtype=np.int32))\n",
    "    return triton_client.infer(model_name=model_name, inputs=[input0, input1], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b22ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = run_random_inference()\n",
    "output.as_numpy('preds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bfda76",
   "metadata": {},
   "source": [
    "##### Huggingface tokenizer with Triton model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29598a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run an inference with a defined Huggingface tokenizer\n",
    "def run_inference(input_ls, tokenizer=BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=False),  model_name='sentiment_model_pytorch'):\n",
    "    triton_client = grpcclient.InferenceServerClient(url=url,verbose=False)\n",
    "\n",
    "    output_t = tokenizer(input_ls, truncation=True, max_length=seq_length, padding='max_length')\n",
    "    output_d = {k:v for k,v in output_t.items()}\n",
    "    \n",
    "    input0 = grpcclient.InferInput('input_ids', (len(input_ls), seq_length), 'INT32')\n",
    "    input1 = grpcclient.InferInput('attention_mask', (len(input_ls), seq_length), 'INT32')\n",
    "\n",
    "    input0.set_data_from_numpy(np.array(output_d[\"input_ids\"]).astype(np.int32))\n",
    "    input1.set_data_from_numpy(np.array(output_d[\"attention_mask\"]).astype(np.int32))\n",
    "    \n",
    "    output = grpcclient.InferRequestedOutput('preds')\n",
    "    \n",
    "    return output_d, triton_client.infer(model_name=model_name, inputs=[input0, input1], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda39bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_ls = ['The product is great', 'This product is bad','This product is good', 'This product is really bad']*250\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "tokens, output = run_inference(log_ls, tokenizer)\n",
    "#output.as_numpy('preds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cede85e",
   "metadata": {},
   "source": [
    "##### Huggingface tokenizer on the Triton server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3c3416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "log_ls = ['The product is great', 'This product is bad','This product is good', 'This product is really bad']*1\n",
    "\n",
    "def send_inference_request_hugf(txt_ls, model_name='end_to_end_pytorch_hugf'):\n",
    "    triton_client = grpcclient.InferenceServerClient(url=url,verbose=False)\n",
    "    input_grpc = grpcclient.InferInput(\"product_reviews\", shape=(1,len(txt_ls)), datatype=\"BYTES\")\n",
    "    input_grpc.set_data_from_numpy(np.asarray([txt_ls], dtype=object))\n",
    "    \n",
    "    \n",
    "    outputs = []\n",
    "    outputs.append(grpcclient.InferRequestedOutput('preds'))\n",
    "    \n",
    "    output = triton_client.infer(model_name=model_name,\n",
    "                               inputs=[input_grpc],\n",
    "                              outputs=outputs)\n",
    "    \n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb9281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 256\n",
    "sentence = 'The product is great'* 500\n",
    "log_ls = [sentence] * 100\n",
    "#log_ls = ['The product is great', 'This product is bad','This product is good', 'This product is really bad']*250\n",
    "#output = send_inference_request_hugf(log_ls)\n",
    "#output.as_numpy('preds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0877b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for cudf tokenizer\n",
    "log_ls_r = [l.encode('utf-8') for l in log_ls]\n",
    "log_ar = np.array(log_ls_r).reshape(1,len(log_ls_r))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b802b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = send_inference_request_hugf(log_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c8479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = send_inference_request(log_ar,'end_to_end_pytorch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c76b6c",
   "metadata": {},
   "source": [
    "##### RAPIDS vs Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971fb165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input list: inout to huggingface tokenizer\n",
    "log_ls = ['The product is great', 'This product is bad','This product is good', 'This product is really bad']*250\n",
    "\n",
    "\n",
    "# Input for cudf tokenizer\n",
    "log_ls_r = [l.encode('utf-8') for l in log_ls]\n",
    "log_ar = np.array(log_ls_r).reshape(1,len(log_ls_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f22f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "tokens, output = run_inference(log_ls, tokenizer)\n",
    "#output.as_numpy('preds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e09a35c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
